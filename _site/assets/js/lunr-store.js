var store = [ 
    
    
    { 
        "url": "/content/1_intro_definitions.html",
        "title": "Introduction",
        "text": "Hello, my name is Andrew Weymouth and I have worked with the University of Idaho Library as the Digital Initiatives Librarian in the Center for Digital Inquiry and Learning (CDIL) department since the fall of 2023. My work generally consists of creating and maintaining our digital collections, working with CDIL fellows, helping to rethink processes and introducing new digital scholarship tools to the department &#10042; Walkthrough of the University of Idaho's Digital Collections page and CollectionBuilder functionalities CollectionBuilder The platform we host our collections on at the U of I is called CollectionBuilder, an “open source framework for creating digital collections and exhibit websites that are driven by metadata and modern static web technology”. This is also the platform we use to host our Taylor Wilderness Research Station Archive collection that I’ll be focusing on today. &#10042;"
    },
    { 
        "url": "/content/2_taylor.html",
        "title": "Taylor Wilderness Research Station",
        "text": "The future site of the Taylor Wilderness Research Station, owned by Dave Lewis, ca. 1937, courtesy U of I Special Collections (IntermtnPics55) Taylor is a secluded facility in the heart of the Frank Church-River of No Return Wilderness in central Idaho, which the University has owned and managed since 1970. Surrounded by over two million acres of wilderness, the facility has been an invaluable resource for wildlife surveys, rangeland monitoring and plant identification. A few years ago, the CDIL was contacted by the people who run the station and were asked to digitize around 2000 documents that were vulnerable to deterioration. The documents were wide ranging, consisting of correspondences, memos and reports as well as hundreds of research papers that were the result of academic residencies at the station. Taylor management wanted to retain the physical documents, so the collection was never formally processed by our Special Collections department, presenting challenges that I’ll discuss in a moment. In a separate project, also before my time at the university, CDIL Fellow Jack Kredell visited Taylor (which can only be hiked or flown into on small backwoods planes) and interviewed subjects on-location. In Jack’s words: \"The oral history project explores both the social and ecological dimensions of Taylor Wilderness Research Station through recorded interviews with past and present researchers, teachers, station managers, and students\" with \"interviews focused on uncovering often-ignored social and experiential content behind scientific pursuit, offering a narrative index of social-ecological change within a wilderness environment.\" Under the guidance of Devin Becker, Associate Dean Research and Instruction for the library, the goal of this project was to create a collection combining these two resources which would not only give access to all of the scientific research that came out of the station but also provide interpretive material that would help visitors better understand the historic, human and geographic context of the institution. &#10042; The Pack Creek Visitor Study Preliminary Report, an example of the types of natural science documents that make up the majority of the Taylor collection, courtesy U of I Special Collections (b06-WildernessRecreation31) Metadata Standardization and Vetting Both of these projects had been dormant for some time, due to some detailed metadata work that needed to be done before moving forward. This work involved standardizing author, place names and subjects, as well as surveying the collection for duplicates, and sometimes triplicates due to the “accession” never being formally processed by an archivist. Special Collections would have caught these redundancies along with sensitive information I later identified, such as applications containing social security numbers and home addresses of former student workers. Documenting this process has enabled us to assess our procedures at CDIL for projects that are part of our digital collections but not formally accessioned. These repositories create an opportunity to highlight unique or at-risk community archive materials across our North Idaho region, while allowing these smaller institutions to maintain permanent stewardship. That said, they also create challenges regarding processing and digital preservation that we are still developing best practices for within the department. &#10042;"
    },
    { 
        "url": "/content/3_bibliometrics.html",
        "title": "Bibliometrics",
        "text": "Once these metadata issues were resolved, I was able to start on the first digital scholarship aspect of the project. Bibliometrics are the use of statistical methods to analyze books, articles and other publications. The concept was to see if there was some way to visualize all of the academic work that came out of Taylor to convey a sense of the institution’s “research impact” over time. Using Web of Science, I was faced with my first question around balancing the validity and the sufficiency of data that you often encounter in these types of projects. In this case, I found that searching for specific names of the scientific papers in Web of Science wasn’t yielding many results – mainly due to their data focusing heavily towards papers published in the late 90s onward, with the majority of the Taylor archive papers published in the 70s and 80s largely absent. &#10042; Arlow Lewis, the caretaker of Taylor Ranch for several years, far left with glasses. Kneeling on left is Jim Bennett, then a PhD student working on bighorn sheep. Carol Bennett is in the yellow top, courtesy U of I Special Collections (DSC_0360) Approach This led me to consider some questions: What does it mean for a paper to be the direct product of a specific place? Does a work need to mention Taylor by name? Are we only looking at research produced by U of I? How long after the author visits Taylor is it still considered a product of that place? With this in mind, I decided to zoom out a bit and look at authors from U of I who were simply present in the archive, rather than specific titles. &#10042; Network Visualization After creating an author name equation so I only needed to search for authors in seven batches of 100 names rather than a bleak 737 individual searches, this compiled Web of Science data was then exported in CSV format and then imported into VOSviewer, a Dutch, open-source network visualization platform with excellent documentation and an intuitive interface. Visualization of citation impact over time of authors that contributed to work found in the Taylor Wilderness Research Station digital archive. I wanted to analyze: future citations of specific documents co-authorship of papers, and fields of interest over time. &#10042; Once these three visualizations were completed, further customization was done adjusting node color to represent scientific discipline, size to represent the number of times the work has been cited, and arrangement according to when they were published. Selecting a node brings up a pane on the lower right hand corner of the screen which reveals more details on the title and, if available, creates a link to the research paper itself. These visualizations were then embedded into our CollectionBuilder site using the VOS-Viewer Online function. While these do take a short while to process the JSON data on loading, the bibliometrics add a colorful and interactive element that reduces a daunting amount of scholarly work into a single visualization. &#10042;"
    },
    { 
        "url": "/content/4_transcription.html",
        "title": "Transcription and Tagging",
        "text": "Another challenge of the project was the transcription element. The oral history videos consisted of 15 interviews ranging from an hour and a half to about a half hour. The person conducting the interviews included long pauses and a lot of informal speech, indicating the footage was originally meant to be edited down, but it never was, rendering the material difficult to follow if someone is listening to the recordings linearly. The CDIL’s Oral History as Data or OHD offered a solution in the form of a fully visualized transcription that could be navigated by tagging and keyword searching. One challenge was that this amount of material would generally take student workers about two semesters to complete with the institution’s previous transcription methods. Building off of my work with the Washington State digital encyclopedia HistoryLink, I experimented with a workflow implementing Adobe Premiere’s transcription tools, which excelled in identifying different speakers, was more accurate than previous tools and could be exported in almost the exact CSV format we needed for OHD. The software also has the advantage of offering a variety of different language packs, which we recently utilized for a Hispanic oral history collection, successfully recognizing speakers frequently jumping between Spanish and English. &#10042; Visualization of Workflow Workflow After testing a few approaches, I was able to find a method that could transcribe an hour and a half long video in around an hour; five minutes to download, ten minutes to process in Premiere, five minutes to format the CSV in Google Sheets and, playing at 2x speed, proof the entire recording for errors in 45 minutes. This proofing and editing can be done ergonomically within Premier’s playback interface or directly into the transcript CSV in Visual Studio Code depending on the severity of the edits needed. In the case of these fifteen videos, which were high fidelity (though featuring some tricky Idaho accents) I’d estimate around a 98% accuracy in Premier’s machine learning. &#10042; Voyant interface with the uncleaned language and distinctive words category mentioned in the tagging process Tagging Transcripts with Text Mining Another strength of OHD is the tagging capability, where threads can be woven through multiple interviews, revealing underlying themes. To efficiently identify tag material, I dropped all of the transcript CSVs into the freely available text mining engine Voyant. List of words extracted from Voyant with various repeating themes highlighted Normally, this type of text mining analysis would require some amount of “cleaning” to remove all of the filler words from a document, but pulling from the “distinctive words” section of the summary can provide a similar snapshot. Copying this data from all fifteen transcriptions, I identified repeating themes, created a synonym list for those tags, searched each document for these words, and added the appropriate tag into the column next to where that dialogue appears in the transcript CSV. Since working on this project, I’ve developed this process, moving away from Voyant and building a text mining tool from scratch using Python with results I’m excited to write about and expand on in future work. &#10042;"
    },
    { 
        "url": "/content/5_mapping.html",
        "title": "Mapping Oral History",
        "text": "At the beginning of this project, reviewing the memos, correspondences and scientific papers in the archive, I was having a difficult time understanding the physical space of Taylor. The area was remote not only physically but conceptually — to the point that there wasn’t a full photo of the station in the almost 2,000 items included in the archive! The only reason I can think of why this was the case is classic scientific rationalism: If we aren’t studying the research station, why would we bother to take a picture of it? This feels like an important note for this conference and how we as librarians approach presenting natural science research, to be mindful in provide broader context to the often magnified scope of the work. &#10042; Google Earth Studio interface with an overhead view on the left, camera view on the right and timeline on the bottom Google Earth Studios To help provide visitors to the collection with ‘a sense of place,’ as one of the interviewees describes it, I began working with Google Earth Studios to animate excerpts of the oral histories. The interface very closely resembles AfterEffects, but in addition to the x y axis, there are also latitude and longitudinal data points and some very impressive chronological lighting controls, which helped add context to one of the stories that took place over four days. &#10042; After a few iterations, these videos were embedded at the top and bottom of the landing page, hopefully providing visitors with an engaging introduction to the landscape they’ll be exploring in different ways throughout the site. &#10042; Conclusion I found the opportunity to work on the Taylor archive a beneficial exercise in: Approaching a digital collection’s materials from the perspective of future visitors Being self aware of my own difficulties in comprehending the subject matter Finding the most appropriate and dynamic tools to make those concepts more clear to myself and hopefully others. Thank you for your time."
    },
    { 
        "url": "/",
        "title": "Home",
        "text": "Presentation for UCLA Library's Captivity: Assembling Nature’s Histories, May 2024 Slides Site (in development) Registration and Full Schedule UCLA Livestream, Friday, May 17, 2024, 12:10pm Contents: Intro Taylor Bibliometrics Transcription Mapping Content: CC BY-NC-ND 4.0 Andrew Weymouth 2024 (get source code). Theme: Variation on workshop-template-b by evanwill"
    }];
